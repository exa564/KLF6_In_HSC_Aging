{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I) Overview of ChIPseq Analysis\n",
    "This pipeline is for SE ChIPseq data\n",
    "This pipeline will do the following:\n",
    "\n",
    "    1) Raw read QC (FastQC)\n",
    "    2) Adapter trimming (cutadapt)\n",
    "    3) Alignment (bowtie2)\n",
    "    4) Mark duplicates (picard)\n",
    "    5) Filtering to remove:\n",
    "            #reads mapping to blacklisted regions (SAMtools, BEDTools)\n",
    "            #reads that are marked as duplicates (SAMtools)\n",
    "            #reads that arent marked as primary alignments (SAMtools)\n",
    "            #reads that are unmapped (SAMtools)   \n",
    "            #reads that map to multiple locations (SAMtools)\n",
    "            #reads containing > 4 mismatches (SAMTools)\n",
    "    6) MultiQC\n",
    "    7) Create normalised bigWig files of the log2(IP/Input) (deepTools2)\n",
    "    8) Call narrow peaks and generate bigWig files of the fold-enrichment of IP/Input (MACS2)\n",
    "    9) Make a config file to use with for the differential peak calling with Diffbind.\n",
    "    10) Generate pooled bigWig files of the log2(IP/Input) to use for heatmaps (deepTools2)\n",
    "\n",
    "Input files:\n",
    "   - fastq.gz files, one per each sample. This pipeline assumes that every IP has an Input\n",
    "   - A config file. The config file is a tab seperated .txt file that contains 5 columns:\n",
    "        - \"seq_id\": the initial sample id\n",
    "        - \"condition\": sample type\n",
    "        - \"replicate\": numeric\n",
    "        - \"mark\": The histone modication profiled, or Input\n",
    "        - \"sample\": new sample name\n",
    "   - blacklist. Got hg19 from: https://github.com/Boyle-Lab/Blacklist/tree/master/lists\n",
    "\n",
    "The working directory is \"chipseq_data\" and the genome path is /genome/hg19/hg19, change if using a different genome. Load both directories as volumes when launching Docker.\n",
    "\n",
    "The hg19.chrom.sizes and blacklist region must be in the working directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from multiprocessing.pool import Pool\n",
    "import os\n",
    "import pybedtools\n",
    "from pybedtools import BedTool\n",
    "import itertools\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II) Read in files and format config\n",
    "- make group column (join mark + condition. eg CTRL_H3K4me3)\n",
    "- make processed name: will be group + replicate + mark : sgNTC_1_H3K4me3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = pd.read_csv('config_file.txt', sep=\"\\t\")\n",
    "config['group']=config['condition'] + \"_\" + config['mark']\n",
    "config['processed_name']=config['condition'] + \"_\" + config['sample'] + \"_\" + config['mark']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify all paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome_path=\"/genome/hg19/hg19\" # Change if need different genome/species\n",
    "blacklist_filter= \"/chipseq_data/blacklist_filter.bed\"\n",
    "fastq_path=\"/chipseq_data/fastq/\"\n",
    "working_dir=\"/chipseq_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'/Reference/KLF6/CRISPRa/ChIPseq/Deeptools/pooled/log2bw/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make any directories that will be needed for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UMR_dir = 'UMR'\n",
    "aligned_dir = 'Aligned_bam'\n",
    "macs2_dir = 'macs2'\n",
    "Deeptools_dir ='Deeptools'\n",
    "Deeptools_log_dir='log2bw'\n",
    "pooled_dir = 'pooled'\n",
    "pooled_UMR_dir = 'UMR'\n",
    "\n",
    "\n",
    "UMR_path = os.path.join(working_dir, UMR_dir)\n",
    "aligned_path = os.path.join(working_dir, aligned_dir)\n",
    "macs2_path = os.path.join(working_dir, macs2_dir)\n",
    "Deeptools_path = os.path.join(working_dir, Deeptools_dir)\n",
    "Deeptools_log_path = os.path.join(Deeptools_dir, Deeptools_log_dir)\n",
    "pooled_path = os.path.join(working_dir, pooled_dir)\n",
    "pooled_UMR_path = os.path.join(pooled_dir, pooled_UMR_dir)\n",
    "pooled_Deep_path = os.path.join(pooled_dir, Deeptools_dir, Deeptools_log_dir)\n",
    "\n",
    "\n",
    "os.makedirs(UMR_path)\n",
    "os.makedirs(aligned_path)\n",
    "os.makedirs(macs2_path)\n",
    "os.makedirs(Deeptools_path)\n",
    "os.makedirs(Deeptools_log_path)\n",
    "os.makedirs(pooled_path)\n",
    "os.makedirs(pooled_UMR_path)\n",
    "os.makedirs(pooled_Deep_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III) Main function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change to False in order to run!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG=True\n",
    "#DEBUG=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chipseq(x):\n",
    "    \n",
    "    read_length = 100 # change as appropriate\n",
    "    \n",
    "    # 1) Perform fastqc on fastq files\n",
    "    \n",
    "    fastq_fullpath=fastq_path + x['seq_id'] + '.fastq.gz'\n",
    "    cmd1 = f'fastqc  --noexkst -f fastq -t 5 {fastq_fullpath}'\n",
    "\n",
    "    print ('=' * 30)\n",
    "    print (cmd1)\n",
    "    if DEBUG == False:\n",
    "        os.system(cmd1)\n",
    "        \n",
    "        \n",
    "    # 2) Trim adapters and remove fragments <25 bp\n",
    "    \n",
    "    name=x['seq_id']\n",
    "    cmd2 = f'cutadapt -l {read_length} -a AAGATCGGAAGAGCACACGTCTGAACTCCAGTCA -m 25 -o {fastq_path}{name}_trim.fastq.gz {fastq_fullpath} --cores=1'\n",
    "    \n",
    "    print ('=' * 30)\n",
    "    print (cmd2)\n",
    "    if DEBUG == False:\n",
    "        os.system(cmd2)\n",
    "\n",
    "\n",
    "    # 3) Align trimmed fastq to hg19 using bowtie\n",
    "    #Save's alignment results to a log file\n",
    "    trim_fullpath = fastq_path + name + '_trim.fastq.gz'\n",
    "    aligned_name = aligned_path + \"/\" + x['seq_id'] + '.bam'\n",
    "    cmd3 = f'(bowtie2 -p 5 -x {genome_path} -U {trim_fullpath}  | samtools view -Sb - > {aligned_name}) 2>{name}.log' \n",
    "\n",
    "    print ('=' * 30)\n",
    "    print (cmd3)\n",
    "    if DEBUG == False:\n",
    "        os.system(cmd3)\n",
    "        \n",
    "        \n",
    "    # 4) Coordinate sort .bam files\n",
    "    \n",
    "    aligned_sort_name = aligned_path + \"/\" + x['seq_id'] + '.sorted.bam'\n",
    "    cmd4 = f'samtools sort {aligned_name} -@ 5 -o {aligned_sort_name}'\n",
    "    \n",
    "    print ('=' * 30)\n",
    "    print (cmd4)\n",
    "    if DEBUG == False:\n",
    "        os.system(cmd4)\n",
    "        \n",
    "        \n",
    "    # 5) Index sorted .bam files\n",
    "    cmd5 = f'samtools index {aligned_sort_name}'\n",
    "\n",
    "    print ('=' * 30)\n",
    "    print (cmd5)\n",
    "    if DEBUG == False:\n",
    "        os.system(cmd5)\n",
    "    \n",
    "        \n",
    "    # 6) Mark duplicates. \n",
    "    \n",
    "    aligned_dup_name = aligned_path + \"/\" + x['seq_id'] + '.dedup.bam'\n",
    "    marked_dup_metrics = x['seq_id'] + '_marked_dup_metrics.txt'\n",
    "    cmd6 = f'picard MarkDuplicates I={aligned_sort_name} O={aligned_dup_name} M={marked_dup_metrics} ASSUME_SORTED=true  REMOVE_DUPLICATES=false VALIDATION_STRINGENCY=LENIENT'\n",
    "\n",
    "    print ('=' * 30)                \n",
    "    print (cmd6)\n",
    "    if DEBUG == False:\n",
    "        os.system(cmd6)\n",
    "    \n",
    "    \n",
    "    # 7) Filter reads\n",
    "    # Will filter out multimapped reads\n",
    "    # Will filter out reads with quality less than 5 (-q 5). see http://dcc.blueprint-epigenome.eu/#/md/chip_seq_grch38\n",
    "    # Will retain only mapped reads (-F 4)\n",
    "    \n",
    "    intermediate_bam = aligned_path + \"/\" + x['seq_id'] + '.intermediate.output.bam'\n",
    "    cmd7 = f'samtools view -b -F 4 -q 5 {aligned_dup_name} > {intermediate_bam} && rm {aligned_dup_name}'\n",
    "    \n",
    "    print ('=' * 30)                \n",
    "    print (cmd7)\n",
    "    if DEBUG == False:\n",
    "        os.system(cmd7)\n",
    "\n",
    "    # 8) Remove duplicate reads  \n",
    "    # Will filter out opitcal duplicates (Flag 1024)\n",
    "\n",
    "    UMR_bam = UMR_path + \"/\" + x['seq_id'] + '.bam'\n",
    "    cmd8 = f'samtools view -b -F 1024 -L {blacklist_filter} {intermediate_bam} > {UMR_bam} && rm {intermediate_bam}' \n",
    "    \n",
    "    print ('=' * 30)                \n",
    "    print (cmd8)\n",
    "    if DEBUG == False:\n",
    "        os.system(cmd8)\n",
    "   \n",
    "        \n",
    "    # 9) Name sort the unique mapped reads\n",
    "    UMR_sort_name= UMR_path + \"/\" + x['seq_id'] + '.sorted.bam'\n",
    "    cmd9 = f'samtools sort {UMR_bam} -@ 5 -o {UMR_sort_name}'\n",
    "    print ('=' * 30)\n",
    "    print (cmd9)\n",
    "    if DEBUG == False:\n",
    "        os.system(cmd9)\n",
    "    \n",
    "    # 10) Index the sorted unique mapped reads\n",
    "\n",
    "    cmd10 = f'samtools index {UMR_sort_name}'\n",
    "\n",
    "    print ('=' * 30)\n",
    "    print (cmd10)\n",
    "    if DEBUG == False:\n",
    "        os.system(cmd10)\n",
    "\n",
    "#     print ('========end========')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSORS = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_dup =Pool(PROCESSORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_dup.map(run_chipseq, [config.iloc[x] for x in range(config.shape[0])] ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) MultiQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!multiqc /chipseq_data/fastq/*_fastqc.zip --export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV) Generate log2 bigwigs\n",
    "- These will be used for plotting heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['sample_condition'] = config['condition'] + \"_\" + config['sample'] \n",
    "config_ip = config.query('mark != \"Input\"').copy()\n",
    "config_input = config.query('mark == \"Input\"').copy()\n",
    "dict_samples_input = dict(zip(config_input['sample_condition'], config_input['seq_id']))\n",
    "config_ip['input_id'] = config_ip['sample_condition'].apply(lambda x : dict_samples_input[x] if x in dict_samples_input else 'missing_wait_to_finish' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_ip['bamReads'] = '/chipseq_data/UMR/' + config_ip['seq_id']  + '.sorted.bam'\n",
    "config_ip['bamControl'] = '/chipseq_data/UMR/' + config_ip['input_id']  + '.sorted.bam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_ip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in config_ip.iterrows():\n",
    "    dir_loc= Deeptools_log_path + \"/\"\n",
    "    config_ip['bamReads'] = UMR_dir + config_ip['seq_id']  + '.sorted.bam'\n",
    "    config_ip['bamControl'] = UMR_dir + config_ip['input_id']  + '.sorted.bam'\n",
    "\n",
    "    ip= row['bamReads']\n",
    "    input_f = row['bamControl']\n",
    "    name= row['processed_name']\n",
    "    \n",
    "    cmd1= f'cd {dir_loc}' \n",
    "    cmd2 = f'bamCompare --bamfile1 {ip} --bamfile2 {input_f} --outFileName {name}_log2.bw --operation log2  -p 15 --outFileFormat bigwig --scaleFactorsMethod readCount'\n",
    "    cmd_to_run = cmd1 + \";\" + cmd2\n",
    "    \n",
    "    #print(cmd_to_run)\n",
    "    !$cmd_to_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V) macs2 peak calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_macs_callpeak(x):\n",
    "    UMR_dir = UMR_path + \"/\"\n",
    "    ip = UMR_dir + x['seq_id']  + '.sorted.bam'\n",
    "    input_f = UMR_dir + x['input_id']  + '.sorted.bam'\n",
    "    name = x['processed_name']\n",
    "    macs_dir= macs2_path + \"/\"\n",
    "    \n",
    "    bdg_file=x['processed_name'] +'_FE.bdg'\n",
    "    bdg_sort=x['processed_name'] +'_FE.sort.bdg'\n",
    "    bw_file= macs_dir + x['processed_name'] +'_FE.bw'\n",
    "\n",
    "    cmd_cd = f'cd {macs_dir}'\n",
    "    cmd1 = f'macs2 callpeak -t {ip} -c {input_f} -f BAM -B -n {name} -g hs --nomodel -q 0.0001 --SPMR --keep-dup all --extsize 200'\n",
    "    cmd2 = f'macs2 bdgcmp -t {name}_treat_pileup.bdg -c {name}_control_lambda.bdg -o {name}_FE.bdg -m FE'.format(name=x[0])\n",
    "    cmd3 = f'cd {macs_dir}; sort -k1,1 -k2,2n {bdg_file} > {bdg_sort}; /UCSC/bedGraphToBigWig {bdg_sort} /chipseq_data/hg19.chrom.sizes {bw_file}'\n",
    "    cmd_to_run = cmd_cd + ';' + cmd1 + ' ; ' + cmd2 + ' ; ' + cmd3\n",
    "\n",
    "    print(cmd_to_run)\n",
    "    \n",
    "    if DEBUG == False:\n",
    "        os.system(cmd_to_run)\n",
    "        return(cmd_to_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_dup_macs =Pool(PROCESSORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_dup_macs.map(run_macs_callpeak, [config_ip.iloc[x] for x in range(config_ip.shape[0])] ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI) Make sample file for diffbind:\n",
    "\n",
    "Sample sheet format according to diffbind:\n",
    "\n",
    "data frame containing sample sheet, or file name of sample sheet to load (ignored if DBA is specified). Columns names in sample sheet may include:\n",
    "\n",
    "    SampleID: Identifier string for sample\n",
    "\n",
    "    Tissue: Identifier string for tissue type\n",
    "\n",
    "    Factor: Identifier string for factor\n",
    "\n",
    "    Condition: Identifier string for condition\n",
    "\n",
    "    Treatment: Identifier string for treatment\n",
    "\n",
    "    Replicate: Replicate number of sample\n",
    "\n",
    "    bamReads: file path for bam file containing aligned reads for ChIP sample\n",
    "\n",
    "    bamControl: file path for bam file containing aligned reads for control sample\n",
    "\n",
    "    ControlID: Identifier string for control sample\n",
    "\n",
    "    Peaks: path for file containing peaks for sample. format determined by PeakCaller field or caller parameter\n",
    "\n",
    "    PeakCaller: Identifier string for peak caller used. If Peaks is not a bed file, this will determine how the Peaks file is parsed. If missing, will use default peak caller specified in caller parameter. Possible values:\n",
    "\n",
    "        “raw”: text file file; peak score is in fourth column\n",
    "\n",
    "        “bed”: .bed file; peak score is in fifth column\n",
    "\n",
    "        “narrow”: default peak.format: narrowPeaks file\n",
    "\n",
    "        “macs”: MACS .xls file\n",
    "\n",
    "        “swembl”: SWEMBL .peaks file\n",
    "\n",
    "        “bayes”: bayesPeak file\n",
    "\n",
    "        “peakset”: peakset written out using pv.writepeakset\n",
    "\n",
    "        “fp4”: FindPeaks v4\n",
    "\n",
    "    PeakFormat: string indicating format for peak files; see PeakCaller and dba.peakset\n",
    "\n",
    "    ScoreCol: column in peak files that contains peak scores\n",
    "\n",
    "    LowerBetter: logical indicating that lower scores signify better peaks\n",
    "\n",
    "    Counts: file path for externally computed read counts; see dba.peakset (counts parameter)\n",
    "\n",
    "For sample sheets loaded from a file, the accepted formats are comma-separated values (column headers, followed by one line per sample), or Excel-formatted spreadsheets (.xls or .xlsx extension). Leading and trailing white space will be removed from all values, with a warning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_ip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_diffBind_sheet(x):\n",
    "    UMR_dir = UMR_path + \"/\"\n",
    "    macs_dir= macs2_path + \"/\"\n",
    "\n",
    "    x['bamReads'] = UMR_dir + x['seq_id']  + '.sorted.bam'\n",
    "    x['bamControl'] = UMR_dir + x['input_id']  + '.sorted.bam'\n",
    "    x['peaks_path'] = macs_dir + x['processed_name'] + '_peaks.narrowPeak'\n",
    "    \n",
    "    sampleSheet = pd.DataFrame({'SampleID': x['processed_name'], 'Factor':x['mark'], 'Condition': x['condition'],\n",
    "              'Treatment': x['condition'], 'Replicate': x['replicate'], 'bamReads':x['bamReads'],\n",
    "             'bamControl' : x['bamControl'], 'ControlID':x['input_id'], 'Peaks' : x['peaks_path'], 'PeakCaller': 'macs'})\n",
    "    print(sampleSheet)\n",
    "    sampleSheet.to_csv('diffBind_sampleSheet.csv', sep = ',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_diffBind_sheet(config_ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VII) Bigwigs using pooled replicates\n",
    "- Use these for heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Generate pooled bams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bam_pool(x):\n",
    "        pool_bam_dir = pooled_UMR_path + \"/\"\n",
    "        UMR_dir = UMR_path + \"/\"\n",
    "\n",
    "        reps_g = x.groupby('group')\n",
    "        for k, v in reps_g:\n",
    "            UMR_sort_name= UMR_dir + v['seq_id'] + '.sorted.bam'\n",
    "            pool_sort_name= pool_bam_dir + k + '.sorted.bam'\n",
    "            files_to_merge = ' '.join(UMR_sort_name.values.tolist())\n",
    "            pool_name = pool_bam_dir + k + '.bam'\n",
    "            cmd12 = f'samtools merge {pool_name} {files_to_merge} && samtools sort {pool_name} -@ 5 -o {pool_sort_name} && samtools index {pool_sort_name} && rm {pool_name}'\n",
    "\n",
    "            !$cmd12\n",
    "            print(cmd12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bam_pool(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Make log2 bigwigs for the pooled bams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_ip['poolBamReads'] = pooled_UMR_path + \"/\" + config_ip['group']  + '.sorted.bam'\n",
    "config_ip['poolBamControl'] = pooled_UMR_path + \"/\" + config_ip['condition']  + '_Input.sorted.bam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in config_ip.iterrows():\n",
    "    dir_loc= pooled_Deep_path + \"/\"\n",
    "\n",
    "    ip= row['poolBamReads']\n",
    "    input_f = row['poolBamControl']\n",
    "    name= row['poolBamReads']\n",
    "    \n",
    "    cmd1= f'cd {dir_loc}' \n",
    "    cmd2 = f'bamCompare --bamfile1 {ip} --bamfile2 {input_f} --outFileName {name}_log2.bw --operation log2  -p 15 --outFileFormat bigwig --scaleFactorsMethod readCount'\n",
    "    cmd_to_run = cmd1 + \";\" + cmd2\n",
    "    \n",
    "    print(cmd_to_run)\n",
    "    !$cmd_to_run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
